{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed training with Amazon SageMaker\n",
    "\n",
    "In this notebook we use the SageMaker Python SDK to setup and run a distributed training job.\n",
    "SageMaker makes it easy to train models across a cluster containing a large number of machines, without having to explicitly manage those resources. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** Import essentials packages, start a sagemaker session and specify the bucket name you created in the pre-requsites section of this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "#bucket_name = 'tfworld2019-<your_bucket_name>'\n",
    "bucket_name = 'tfworld2019'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** Specify hyperparameters, instance type and number of instances to distribute training to. The `hvd_processes_per_host` corrosponds to number of GPUs per instances. \n",
    "For example, if you choose:\n",
    "```\n",
    "hvd_instance_type = 'ml.p3.8large'\n",
    "hvd_instance_count = 2\n",
    "hvd_processes_per_host = 4\n",
    "```\n",
    "\n",
    "Since p3.8xlarge instance has 4 GPUs, we'll we distributing training to 8 workers, 1 per GPU.\n",
    "This is spread across 2 instances (or nodes). SageMaker automatically takes care of spinning up these instances and making sure they can communiate with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {'epochs': 100, \n",
    "                   'learning-rate': 0.001,\n",
    "                   'momentum': 0.9,\n",
    "                   'weight-decay': 2e-4,\n",
    "                   'optimizer': 'adam',\n",
    "                   'batch-size' : 256}\n",
    "\n",
    "hvd_instance_type = 'ml.c5.xlarge'\n",
    "hvd_instance_count = 2\n",
    "hvd_processes_per_host = 1\n",
    "\n",
    "print('Distributed training with a total of {} workers'.format(hvd_processes_per_host*hvd_instance_count))\n",
    "print('{} x {} instances with {} processes per instance'.format(hvd_instance_count, hvd_instance_type, hvd_processes_per_host))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** In this cell we create a SageMaker estimator, by providing it with all the information it needs to launch instances and execute training on those instances.\n",
    "\n",
    "Since we're using horovod for distributed training, we specify `distributions` to mpi which is used by horovod.\n",
    "\n",
    "In the TensorFlow estimator call, we specify training script under `entry_point` and dependencies under `code`. SageMaker automatically copies these files into a TensorFlow container behind the scenes, and are executed on the training instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "output_path = 's3://{}/'.format(bucket_name)\n",
    "job_name = 'sm-dist-{}x{}-workers-'.format(hvd_instance_count, hvd_processes_per_host) + time.strftime('%Y-%m-%d-%H-%M-%S-%j', time.gmtime())\n",
    "model_dir = output_path + 'tensorboard_logs/' + job_name\n",
    "\n",
    "distributions = {'mpi': {\n",
    "                    'enabled': True,\n",
    "                    'processes_per_host': hvd_processes_per_host,\n",
    "                    'custom_mpi_options': '-verbose --NCCL_DEBUG=INFO -x OMPI_MCA_btl_vader_single_copy_mechanism=none'\n",
    "                        }\n",
    "                }\n",
    "\n",
    "estimator_hvd = TensorFlow(base_job_name='hvd-cifar10-tf',\n",
    "                       source_dir='code',\n",
    "                       entry_point='cifar10-multi-gpu-horovod-sagemaker.py', \n",
    "                       role=role,\n",
    "                       framework_version='1.14',\n",
    "                       py_version='py3',\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       train_instance_count=hvd_instance_count, \n",
    "                       train_instance_type=hvd_instance_type,\n",
    "                       output_path=output_path,\n",
    "                       model_dir=model_dir,\n",
    "                       tags = [{'Key' : 'Project', 'Value' : 'cifar10'},{'Key' : 'TensorBoard', 'Value' : 'dist'}],\n",
    "                       metric_definitions=[{'Name': 'val_acc', 'Regex': 'val_acc: ([0-9\\\\.]+)'}],\n",
    "                       distributions=distributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4:** Specify dataset locations in Amazon S3 and then call the fit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 's3://{}/cifar10-dataset/train'.format(bucket_name)\n",
    "val_path = 's3://{}/cifar10-dataset/validation'.format(bucket_name)\n",
    "eval_path = 's3://{}/cifar10-dataset/eval/'.format(bucket_name)\n",
    "\n",
    "estimator_hvd.fit({'train': train_path,'validation': val_path,'eval': eval_path}, \n",
    "                  job_name=job_name, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5:** Monitor progress on TensorBoard. Launch tensorboard and open the link on a new tab to visualize training progress, and navigate to the following link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorBoard 1.14.0 at http://ip-172-16-89-111:6006/ (Press CTRL+C to quit)\n",
      "W1028 20:55:37.536751 140564607526656 core_plugin.py:172] Unable to get first event timestamp for run sm-dist-1x1-gpu-instances2019-10-24-10-08-55-297: No event timestamp could be found\n",
      "W1028 20:55:37.777247 140564607526656 core_plugin.py:172] Unable to get first event timestamp for run sm-dist-1x8-gpu-instances2019-10-24-07-43-40-297: No event timestamp could be found\n",
      "W1028 20:55:37.984411 140564607526656 core_plugin.py:172] Unable to get first event timestamp for run sm-dist-2x1-gpu-instances2019-10-28-10-24-06-301: No event timestamp could be found\n",
      "W1028 20:55:38.320934 140564607526656 core_plugin.py:172] Unable to get first event timestamp for run sm-dist-2x1-workers2019-10-28-20-28-23-301: No event timestamp could be found\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!S3_REGION=us-west-2 tensorboard --logdir s3://{bucket_name}/tensorboard_logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a new browser tan and navigate to the folloiwng link to access TensorBoard:\n",
    "<br> https://tfworld2019.notebook.us-west-2.sagemaker.aws/proxy/6006/\n",
    "<br> Make sure that the name of the notebook instance is correct in the link above.\n",
    "<br> Don't forget the slash at the end of the URL 6006/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
