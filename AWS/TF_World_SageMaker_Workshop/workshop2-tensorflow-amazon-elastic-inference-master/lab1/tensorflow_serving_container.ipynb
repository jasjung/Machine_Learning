{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the SageMaker TensorFlow Serving Container\n",
    "\n",
    "The [SageMaker TensorFlow Serving Container](https://github.com/aws/sagemaker-tensorflow-serving-container) makes it easy to deploy trained TensorFlow models to a SageMaker Endpoint without the need for any custom model loading or inference code.\n",
    "\n",
    "In this example, we will show how deploy one or more pre-trained models from [TensorFlow Hub](https://www.tensorflow.org/hub/) to a SageMaker Endpoint using the [SageMaker Python SDK](https://github.com/aws/sagemaker-python-sdk), and then use the model(s) to perform inference requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, we need to ensure we have an up-to-date version of the SageMaker Python SDK, and install a few\n",
    "additional python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U --quiet \"sagemaker>=1.14.2\"\n",
    "!pip install -U --quiet opencv-python tensorflow-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll get the IAM execution role from our notebook environment, so that SageMaker can access resources in your AWS account later in the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and prepare a model from TensorFlow Hub\n",
    "\n",
    "The TensorFlow Serving Container works with any model stored in TensorFlow's [SavedModel format](https://www.tensorflow.org/guide/saved_model). This could be the output of your own training job or a model trained elsewhere. For this example, we will use a pre-trained version of the MobileNet V2 image classification model from [TensorFlow Hub](https://tfhub.dev/).\n",
    "\n",
    "The TensorFlow Hub models are pre-trained, but do not include a serving ``signature_def``, so we'll need to load the model into a TensorFlow session, define the input and output layers, and export it as a SavedModel. There is a helper function in this notebook's `sample_utils.py` module that will do that for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sample_utils\n",
    "\n",
    "model_name = 'mobilenet_v2_140_224'\n",
    "export_path = 'mobilenet'\n",
    "model_path = sample_utils.tfhub_to_savedmodel(model_name, export_path)\n",
    "\n",
    "print('SavedModel exported to {}'.format(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After exporting the model, we can inspect it using TensorFlow's ``saved_model_cli`` command. In the command output, you should see \n",
    "\n",
    "```\n",
    "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
    "\n",
    "signature_def['serving_default']:\n",
    "...\n",
    "```\n",
    "\n",
    "The command output should also show details of the model inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!saved_model_cli show --all --dir {model_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to create a model archive file containing the exported model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a model archive file\n",
    "\n",
    "SageMaker models need to be packaged in `.tar.gz` files. When your endpoint is provisioned, the files in the archive will be extracted and put in `/opt/ml/model/` on the endpoint. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -C \"$PWD\" -czf mobilenet.tar.gz mobilenet/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the model archive file to S3\n",
    "\n",
    "We now have a suitable model archive ready in our notebook. We need to upload it to S3 before we can create a SageMaker Model that. We'll use the SageMaker Python SDK to handle the upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.session import Session\n",
    "\n",
    "model_data = Session().upload_data(path='mobilenet.tar.gz', key_prefix='model')\n",
    "print('model uploaded to: {}'.format(model_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a SageMaker Model and Endpoint\n",
    "\n",
    "Now that the model archive is in S3, we can create a Model and deploy it to an \n",
    "Endpoint with a few lines of python code:\n",
    "\n",
    "### Note: In next cell, read instructions carefully and try different scanerios as instructed. You will have to do  some simple calculations to note the difference in latency and cost for each deployment configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow.serving import Model\n",
    "\n",
    "# Use an env argument to set the name of the default model.\n",
    "# This is optional, but recommended when you deploy multiple models\n",
    "# so that requests that don't include a model name are sent to a \n",
    "# predictable model.\n",
    "env = {'SAGEMAKER_TFS_DEFAULT_MODEL_NAME': 'mobilenet_v2_140_224'}\n",
    "\n",
    "model = Model(model_data=model_data, role=sagemaker_role, framework_version='1.14', env=env)\n",
    "\n",
    "# Try below three scanerios one at a time by uncommenting the predictor = model.deploy*** for each deployment configuration\n",
    "# Calculate the latency and cost for each deployment configurtaion for processed requests\n",
    "\n",
    "# Continue till end for each scanerio and come back and try next scnerio\n",
    "# Note the seconds it takes to process the request in following cells and use the forumla = seconds/60*price per hour of compute configuration for each scanerio\n",
    "# The us-west-2 cost per hour for each configuration is provided in the comment line about each deployment \n",
    "\n",
    "# Scanerio 1 - uncomment predictor below and estimate cost of running deployment with ml.c5.xlarge+ ml.eia1.medium\n",
    "#ml.c5.xlarge price per hour is $0.238 and ml.eia1.medium per per hour is $0.182 i.e total ml.c5.xlarge+ ml.eia1.medium is $0.42\n",
    "#predictor = model.deploy(initial_instance_count=1, instance_type='ml.c5.xlarge', accelerator_type='ml.eia1.medium')\n",
    "\n",
    "# Scanerio 2 - uncomment predictor below and estimate cost of running deployment with ml.p3.2xlarge\n",
    "# ml.p3.2xlarge price per hour is $4.284\n",
    "#predictor = model.deploy(initial_instance_count=1, instance_type='ml.p3.2xlarge')\n",
    "\n",
    "# Scanerio 3 - uncomment predictor below and estimate cost of running deployment with ml.c5.xlarge\n",
    "#ml.c5.xlarge price per hour $0.238\n",
    "#predictor = model.deploy(initial_instance_count=1, instance_type='ml.c5.xlarge')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions using the endpoint\n",
    "\n",
    "The endpoint is now up and running, and ready to handle inference requests. The `deploy` call above returned a `predictor` object. The `predict` method of this object handles sending requests to the endpoint. It also automatically handles JSON serialization of our input arguments, and JSON deserialization of the prediction results.\n",
    "\n",
    "We'll use these sample images:\n",
    "\n",
    "<img src=\"kitten.jpg\" align=\"left\" style=\"padding: 8px;\">\n",
    "<img src=\"bee.jpg\" style=\"padding: 8px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='kitten.jpg')\n",
    "Image(filename='bee.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# read the image files into a tensor (numpy array)\n",
    "kitten_image = sample_utils.image_file_to_tensor('kitten.jpg')\n",
    "\n",
    "# get a prediction from the endpoint\n",
    "# the image input is automatically converted to a JSON request.\n",
    "# the JSON response from the endpoint is returned as a python dict\n",
    "\n",
    "start = time.time()\n",
    "for char in \"I want to loop as many times as characters I have in this sentence.\":\n",
    "#predictions = [get_prediction(image) for image in images]\n",
    " result = predictor.predict(kitten_image)\n",
    "end = time.time()\n",
    "print(end - start,'seconds')\n",
    "\n",
    "# show the raw result\n",
    "print(result)\n",
    "from IPython.display import Image\n",
    "Image(filename='kitten.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add class labels and show formatted results\n",
    "\n",
    "The `sample_utils` module includes functions that can add Imagenet class labels to our results and print formatted output. Let's use them to get a better sense of how well our model worked on the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add class labels to the predicted result\n",
    "sample_utils.add_imagenet_labels(result)\n",
    "\n",
    "# show the probabilities and labels for the top predictions\n",
    "sample_utils.print_probabilities_and_labels(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "\n",
    "To avoid incurring charges to your AWS account for the resources used in this tutorial, you need to delete the SageMaker Endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
