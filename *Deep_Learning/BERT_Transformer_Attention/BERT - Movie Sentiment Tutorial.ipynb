{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Data</a></span></li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Preprocessing</a></span></li><li><span><a href=\"#Creating-a-model\" data-toc-modified-id=\"Creating-a-model-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Creating a model</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT - Predicting Movie Review Sentiment with BERT on TF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_hub\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/00/0e/a91780d07592b1abf9c91344ce459472cc19db3b67fdf3a61dca6ebb2f5c/tensorflow_hub-0.7.0-py2.py3-none-any.whl (89kB)\n",
      "\u001b[K     |████████████████████████████████| 92kB 2.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.4.0 in /Users/ijung/anaconda3/lib/python3.6/site-packages (from tensorflow_hub) (3.6.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /Users/ijung/anaconda3/lib/python3.6/site-packages (from tensorflow_hub) (1.14.3)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/ijung/anaconda3/lib/python3.6/site-packages (from tensorflow_hub) (1.11.0)\n",
      "Requirement already satisfied: setuptools in /Users/ijung/anaconda3/lib/python3.6/site-packages (from protobuf>=3.4.0->tensorflow_hub) (39.1.0)\n",
      "Installing collected packages: tensorflow-hub\n",
      "Successfully installed tensorflow-hub-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-tensorflow\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/66/7eb4e8b6ea35b7cc54c322c816f976167a43019750279a8473d355800a93/bert_tensorflow-1.0.1-py2.py3-none-any.whl (67kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 1.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/ijung/anaconda3/lib/python3.6/site-packages (from bert-tensorflow) (1.11.0)\n",
      "Installing collected packages: bert-tensorflow\n",
      "Successfully installed bert-tensorflow-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from datetime import datetime\n",
    "import sys "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version: 1.12.0\n",
      "Python Version:     3.6\n",
      "Date: 2020-01-08\n"
     ]
    }
   ],
   "source": [
    "print('Tensorflow Version:',tf.__version__)\n",
    "print('Python Version:    ', sys.version[:3])\n",
    "print('Date:',str(datetime.now()).split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir OUTPUT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Model output directory: OUTPUT_DIR *****\n"
     ]
    }
   ],
   "source": [
    "# Set the output directory for saving model file\n",
    "# Optionally, set a GCP bucket location\n",
    "\n",
    "OUTPUT_DIR = 'OUTPUT_DIR'#@param {type:\"string\"}\n",
    "#@markdown Whether or not to clear/delete the directory and create a new one\n",
    "DO_DELETE = False #@param {type:\"boolean\"}\n",
    "#@markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n",
    "\n",
    "if DO_DELETE:\n",
    "    try:\n",
    "        tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
    "    except:\n",
    "    # Doesn't matter if the directory didn't exist\n",
    "        pass\n",
    "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
    "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Load all files from a directory in a DataFrame.\n",
    "def load_directory_data(directory):\n",
    "  data = {}\n",
    "  data[\"sentence\"] = []\n",
    "  data[\"sentiment\"] = []\n",
    "  for file_path in os.listdir(directory):\n",
    "    with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:\n",
    "      data[\"sentence\"].append(f.read())\n",
    "      data[\"sentiment\"].append(re.match(\"\\d+_(\\d+)\\.txt\", file_path).group(1))\n",
    "  return pd.DataFrame.from_dict(data)\n",
    "\n",
    "# Merge positive and negative examples, add a polarity column and shuffle.\n",
    "def load_dataset(directory):\n",
    "  pos_df = load_directory_data(os.path.join(directory, \"pos\"))\n",
    "  neg_df = load_directory_data(os.path.join(directory, \"neg\"))\n",
    "  pos_df[\"polarity\"] = 1\n",
    "  neg_df[\"polarity\"] = 0\n",
    "  return pd.concat([pos_df, neg_df]).sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Download and process the dataset files.\n",
    "def download_and_load_datasets(force_download=False):\n",
    "  dataset = tf.keras.utils.get_file(\n",
    "      fname=\"aclImdb.tar.gz\", \n",
    "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
    "      extract=True)\n",
    "  \n",
    "  train_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                       \"aclImdb\", \"train\"))\n",
    "  test_df = load_dataset(os.path.join(os.path.dirname(dataset), \n",
    "                                      \"aclImdb\", \"test\"))\n",
    "  \n",
    "  return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "84131840/84125825 [==============================] - 9s 0us/step\n"
     ]
    }
   ],
   "source": [
    "train, test = download_and_load_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep training fast\n",
    "train = train.sample(5000)\n",
    "test = test.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sentence', 'sentiment', 'polarity'], dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>9983</td>\n",
       "      <td>well well One cant b wasting time just cause o...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2443</td>\n",
       "      <td>I got this DVD well over 2 years ago and only ...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2375</td>\n",
       "      <td>Flynn, known mostly for his swashbuckling role...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15447</td>\n",
       "      <td>Sure Star Wars (a movie I have seen at least f...</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>A somewhat typical bit of filmmaking from this...</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence sentiment  polarity\n",
       "9983   well well One cant b wasting time just cause o...         2         0\n",
       "2443   I got this DVD well over 2 years ago and only ...         3         0\n",
       "2375   Flynn, known mostly for his swashbuckling role...         9         1\n",
       "15447  Sure Star Wars (a movie I have seen at least f...        10         1\n",
       "238    A somewhat typical bit of filmmaking from this...         8         1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data is the 'sentence' column and our label is the 'polarity' column (0, 1 for negative and positive, respecitvely)\n",
    "DATA_COLUMN = 'sentence'\n",
    "LABEL_COLUMN = 'polarity'\n",
    "# label_list is the list of labels, i.e. True, False or 0, 1 or 'dog', 'cat'\n",
    "label_list = [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transform our data into a format BERT understands.\n",
    "\n",
    "- `text_a` is the text we want to classify, which in this case, is the `Request` field in our Dataframe.\n",
    "- `text_b` is used if we're training a model to understand the relationship between sentences (i.e. is text_b a translation of text_a? Is text_b an answer to the question asked by text_a?). This doesn't apply to our task, so we can `leave text_b blank.`\n",
    "- label is the label for our example, i.e. `True, False`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- guid: Unique id for the example.\n",
    "- text_a: string. The untokenized text of the first sequence. For single sequence tasks, only this sequence must be specified.\n",
    "- text_b: (Optional) string. The untokenized text of the second sequence. Only must be specified for sequence pair tasks.\n",
    "- label: (Optional) string. The label of the example. This should be specified for train and dev examples, but not for test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the InputExample class from BERT's run_classifier code to create examples from the data\n",
    "train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None, # Globally unique ID for bookkeeping, unused in this example\n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)\n",
    "\n",
    "test_InputExamples = test.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n",
    "                                                                   text_a = x[DATA_COLUMN], \n",
    "                                                                   text_b = None, \n",
    "                                                                   label = x[LABEL_COLUMN]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9983     <bert.run_classifier.InputExample object at 0x...\n",
       "2443     <bert.run_classifier.InputExample object at 0x...\n",
       "2375     <bert.run_classifier.InputExample object at 0x...\n",
       "15447    <bert.run_classifier.InputExample object at 0x...\n",
       "238      <bert.run_classifier.InputExample object at 0x...\n",
       "                               ...                        \n",
       "5953     <bert.run_classifier.InputExample object at 0x...\n",
       "4533     <bert.run_classifier.InputExample object at 0x...\n",
       "7443     <bert.run_classifier.InputExample object at 0x...\n",
       "4877     <bert.run_classifier.InputExample object at 0x...\n",
       "22278    <bert.run_classifier.InputExample object at 0x...\n",
       "Length: 5000, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_InputExamples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Lowercase our text (if we're using a BERT lowercase model)\n",
    "2. Tokenize it (i.e. \"sally says hi\" -> [\"sally\", \"says\", \"hi\"])\n",
    "3. Break words into WordPieces (i.e. \"calling\" -> [\"call\", \"##ing\"])\n",
    "4. Map our words to indexes using a vocab file that BERT provides\n",
    "5. Add special \"CLS\" and \"SEP\" tokens (see the readme)\n",
    "6. Append \"index\" and \"segment\" tokens to each input (see the BERT paper)\n",
    "\n",
    "\n",
    "- CLS - classification \n",
    "- SEP - separating sentences \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a path to an uncased (all lowercase) version of BERT\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "\n",
    "def create_tokenizer_from_hub_module():\n",
    "  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "  with tf.Graph().as_default():\n",
    "    bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "    with tf.Session() as sess:\n",
    "      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n",
    "      \n",
    "  return bert.tokenization.FullTokenizer(\n",
    "      vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "\n",
    "tokenizer = create_tokenizer_from_hub_module()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we just learned that the BERT model we're using expects lowercase data (that's what stored in tokenization_info[\"do_lower_case\"]) and we also loaded BERT's vocab file. We also created a tokenizer, which breaks words into word pieces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bert.tokenization.FullTokenizer at 0x1a35e01898>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'here',\n",
       " \"'\",\n",
       " 's',\n",
       " 'an',\n",
       " 'example',\n",
       " 'of',\n",
       " 'using',\n",
       " 'the',\n",
       " 'bert',\n",
       " 'token',\n",
       " '##izer']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:12.998394 4797896128 tf_logging.py:115] Writing example 0 of 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.006582 4797896128 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.008357 4797896128 tf_logging.py:115] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] well well one can ##t b wasting time just cause of a big star - cast . . i think all i could see is a bunch of talents wasting their time on a big screen with some pathetic humor which will appeal to i do not know who ? some pathetic songs that will be heard by who ? some pathetic ##ally abrupt turning ##s justified by who ? race against time ? u mean waste against time ? ok so first you spoil your kid , then you teach him a lesson wow we are so ignorant of this fact whoever said its a brilliant new concept probably is some other species other than human alright fine let me come comment like humans do [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.009505 4797896128 tf_logging.py:115] tokens: [CLS] well well one can ##t b wasting time just cause of a big star - cast . . i think all i could see is a bunch of talents wasting their time on a big screen with some pathetic humor which will appeal to i do not know who ? some pathetic songs that will be heard by who ? some pathetic ##ally abrupt turning ##s justified by who ? race against time ? u mean waste against time ? ok so first you spoil your kid , then you teach him a lesson wow we are so ignorant of this fact whoever said its a brilliant new concept probably is some other species other than human alright fine let me come comment like humans do [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2092 2092 2028 2064 2102 1038 18313 2051 2074 3426 1997 1037 2502 2732 1011 3459 1012 1012 1045 2228 2035 1045 2071 2156 2003 1037 9129 1997 11725 18313 2037 2051 2006 1037 2502 3898 2007 2070 17203 8562 2029 2097 5574 2000 1045 2079 2025 2113 2040 1029 2070 17203 2774 2008 2097 2022 2657 2011 2040 1029 2070 17203 3973 18772 3810 2015 15123 2011 2040 1029 2679 2114 2051 1029 1057 2812 5949 2114 2051 1029 7929 2061 2034 2017 27594 2115 4845 1010 2059 2017 6570 2032 1037 10800 10166 2057 2024 2061 21591 1997 2023 2755 9444 2056 2049 1037 8235 2047 4145 2763 2003 2070 2060 2427 2060 2084 2529 10303 2986 2292 2033 2272 7615 2066 4286 2079 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.011230 4797896128 tf_logging.py:115] input_ids: 101 2092 2092 2028 2064 2102 1038 18313 2051 2074 3426 1997 1037 2502 2732 1011 3459 1012 1012 1045 2228 2035 1045 2071 2156 2003 1037 9129 1997 11725 18313 2037 2051 2006 1037 2502 3898 2007 2070 17203 8562 2029 2097 5574 2000 1045 2079 2025 2113 2040 1029 2070 17203 2774 2008 2097 2022 2657 2011 2040 1029 2070 17203 3973 18772 3810 2015 15123 2011 2040 1029 2679 2114 2051 1029 1057 2812 5949 2114 2051 1029 7929 2061 2034 2017 27594 2115 4845 1010 2059 2017 6570 2032 1037 10800 10166 2057 2024 2061 21591 1997 2023 2755 9444 2056 2049 1037 8235 2047 4145 2763 2003 2070 2060 2427 2060 2084 2529 10303 2986 2292 2033 2272 7615 2066 4286 2079 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.012521 4797896128 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.013615 4797896128 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.014683 4797896128 tf_logging.py:115] label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.023054 4797896128 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.024551 4797896128 tf_logging.py:115] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] i got this dvd well over 2 years ago and only decided to watch it yesterday . i don ' t know why it took me so long as i do like the inspector ga ##dget show and even the new ga ##dget and the ga ##dget ##ini ##s . while it may have a bright color pal ##let and all the technical so ##phi ##stic ##ation of a modern animated movie , there are some old things missing that bog this ga ##dget right down the toilet . < br / > < br / > first of all the classic inspector ga ##dget theme song and music is completely absent . the composer tries to compromise by doing a score that sounds similar but [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.025832 4797896128 tf_logging.py:115] tokens: [CLS] i got this dvd well over 2 years ago and only decided to watch it yesterday . i don ' t know why it took me so long as i do like the inspector ga ##dget show and even the new ga ##dget and the ga ##dget ##ini ##s . while it may have a bright color pal ##let and all the technical so ##phi ##stic ##ation of a modern animated movie , there are some old things missing that bog this ga ##dget right down the toilet . < br / > < br / > first of all the classic inspector ga ##dget theme song and music is completely absent . the composer tries to compromise by doing a score that sounds similar but [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1045 2288 2023 4966 2092 2058 1016 2086 3283 1998 2069 2787 2000 3422 2009 7483 1012 1045 2123 1005 1056 2113 2339 2009 2165 2033 2061 2146 2004 1045 2079 2066 1996 7742 11721 24291 2265 1998 2130 1996 2047 11721 24291 1998 1996 11721 24291 5498 2015 1012 2096 2009 2089 2031 1037 4408 3609 14412 7485 1998 2035 1996 4087 2061 21850 10074 3370 1997 1037 2715 6579 3185 1010 2045 2024 2070 2214 2477 4394 2008 22132 2023 11721 24291 2157 2091 1996 11848 1012 1026 7987 1013 1028 1026 7987 1013 1028 2034 1997 2035 1996 4438 7742 11721 24291 4323 2299 1998 2189 2003 3294 9962 1012 1996 4543 5363 2000 12014 2011 2725 1037 3556 2008 4165 2714 2021 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.026936 4797896128 tf_logging.py:115] input_ids: 101 1045 2288 2023 4966 2092 2058 1016 2086 3283 1998 2069 2787 2000 3422 2009 7483 1012 1045 2123 1005 1056 2113 2339 2009 2165 2033 2061 2146 2004 1045 2079 2066 1996 7742 11721 24291 2265 1998 2130 1996 2047 11721 24291 1998 1996 11721 24291 5498 2015 1012 2096 2009 2089 2031 1037 4408 3609 14412 7485 1998 2035 1996 4087 2061 21850 10074 3370 1997 1037 2715 6579 3185 1010 2045 2024 2070 2214 2477 4394 2008 22132 2023 11721 24291 2157 2091 1996 11848 1012 1026 7987 1013 1028 1026 7987 1013 1028 2034 1997 2035 1996 4438 7742 11721 24291 4323 2299 1998 2189 2003 3294 9962 1012 1996 4543 5363 2000 12014 2011 2725 1037 3556 2008 4165 2714 2021 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.027948 4797896128 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.029200 4797896128 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.030171 4797896128 tf_logging.py:115] label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.042335 4797896128 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.043679 4797896128 tf_logging.py:115] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] flynn , known mostly for his sw ##ash ##buck ##ling roles ( and his bedroom antics ! ) takes a different tack with this film and it works beautifully . playing real - life boxing champ jim corbett , flynn turns on the charm full blast as he makes his way from a stifled san francisco bank teller to a celebrated pu ##gil ##ist , all the while setting one eye on society de ##b smith . he and best pal carson attend an illegal bare - kn ##uck ##le fight and are arrested along with scores of other men ( and a dog ! ) including a prominent judge . the next day , he gets a chance , via smith , to gain entrance [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.044848 4797896128 tf_logging.py:115] tokens: [CLS] flynn , known mostly for his sw ##ash ##buck ##ling roles ( and his bedroom antics ! ) takes a different tack with this film and it works beautifully . playing real - life boxing champ jim corbett , flynn turns on the charm full blast as he makes his way from a stifled san francisco bank teller to a celebrated pu ##gil ##ist , all the while setting one eye on society de ##b smith . he and best pal carson attend an illegal bare - kn ##uck ##le fight and are arrested along with scores of other men ( and a dog ! ) including a prominent judge . the next day , he gets a chance , via smith , to gain entrance [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 13259 1010 2124 3262 2005 2010 25430 11823 24204 2989 4395 1006 1998 2010 5010 27440 999 1007 3138 1037 2367 26997 2007 2023 2143 1998 2009 2573 17950 1012 2652 2613 1011 2166 8362 24782 3958 24119 1010 13259 4332 2006 1996 11084 2440 8479 2004 2002 3084 2010 2126 2013 1037 27146 2624 3799 2924 21322 2000 1037 6334 16405 20142 2923 1010 2035 1996 2096 4292 2028 3239 2006 2554 2139 2497 3044 1012 2002 1998 2190 14412 9806 5463 2019 6206 6436 1011 14161 12722 2571 2954 1998 2024 4727 2247 2007 7644 1997 2060 2273 1006 1998 1037 3899 999 1007 2164 1037 4069 3648 1012 1996 2279 2154 1010 2002 4152 1037 3382 1010 3081 3044 1010 2000 5114 4211 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.046037 4797896128 tf_logging.py:115] input_ids: 101 13259 1010 2124 3262 2005 2010 25430 11823 24204 2989 4395 1006 1998 2010 5010 27440 999 1007 3138 1037 2367 26997 2007 2023 2143 1998 2009 2573 17950 1012 2652 2613 1011 2166 8362 24782 3958 24119 1010 13259 4332 2006 1996 11084 2440 8479 2004 2002 3084 2010 2126 2013 1037 27146 2624 3799 2924 21322 2000 1037 6334 16405 20142 2923 1010 2035 1996 2096 4292 2028 3239 2006 2554 2139 2497 3044 1012 2002 1998 2190 14412 9806 5463 2019 6206 6436 1011 14161 12722 2571 2954 1998 2024 4727 2247 2007 7644 1997 2060 2273 1006 1998 1037 3899 999 1007 2164 1037 4069 3648 1012 1996 2279 2154 1010 2002 4152 1037 3382 1010 3081 3044 1010 2000 5114 4211 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.047268 4797896128 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.048608 4797896128 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.049612 4797896128 tf_logging.py:115] label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.055034 4797896128 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.056777 4797896128 tf_logging.py:115] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] sure star wars ( a movie i have seen at least fifty times ) beats all the others in special effects , but this film has every thing else ! < br / > < br / > it has horror ( non - graphical ) , romance , robots , witty rep ##arte ##e , intelligence , ( surprisingly good ) special effects , and drama . < br / > < br / > i saw this film a couple of years ago in a revival with a newly struck print , and i was amazed at how well it held up today . i thought the old 40 ' s style electronics would look ho ##key , but they somehow looked futuristic and [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.059982 4797896128 tf_logging.py:115] tokens: [CLS] sure star wars ( a movie i have seen at least fifty times ) beats all the others in special effects , but this film has every thing else ! < br / > < br / > it has horror ( non - graphical ) , romance , robots , witty rep ##arte ##e , intelligence , ( surprisingly good ) special effects , and drama . < br / > < br / > i saw this film a couple of years ago in a revival with a newly struck print , and i was amazed at how well it held up today . i thought the old 40 ' s style electronics would look ho ##key , but they somehow looked futuristic and [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2469 2732 5233 1006 1037 3185 1045 2031 2464 2012 2560 5595 2335 1007 10299 2035 1996 2500 1999 2569 3896 1010 2021 2023 2143 2038 2296 2518 2842 999 1026 7987 1013 1028 1026 7987 1013 1028 2009 2038 5469 1006 2512 1011 20477 1007 1010 7472 1010 13507 1010 25591 16360 24847 2063 1010 4454 1010 1006 10889 2204 1007 2569 3896 1010 1998 3689 1012 1026 7987 1013 1028 1026 7987 1013 1028 1045 2387 2023 2143 1037 3232 1997 2086 3283 1999 1037 6308 2007 1037 4397 4930 6140 1010 1998 1045 2001 15261 2012 2129 2092 2009 2218 2039 2651 1012 1045 2245 1996 2214 2871 1005 1055 2806 8139 2052 2298 7570 14839 1010 2021 2027 5064 2246 28971 1998 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.061925 4797896128 tf_logging.py:115] input_ids: 101 2469 2732 5233 1006 1037 3185 1045 2031 2464 2012 2560 5595 2335 1007 10299 2035 1996 2500 1999 2569 3896 1010 2021 2023 2143 2038 2296 2518 2842 999 1026 7987 1013 1028 1026 7987 1013 1028 2009 2038 5469 1006 2512 1011 20477 1007 1010 7472 1010 13507 1010 25591 16360 24847 2063 1010 4454 1010 1006 10889 2204 1007 2569 3896 1010 1998 3689 1012 1026 7987 1013 1028 1026 7987 1013 1028 1045 2387 2023 2143 1037 3232 1997 2086 3283 1999 1037 6308 2007 1037 4397 4930 6140 1010 1998 1045 2001 15261 2012 2129 2092 2009 2218 2039 2651 1012 1045 2245 1996 2214 2871 1005 1055 2806 8139 2052 2298 7570 14839 1010 2021 2027 5064 2246 28971 1998 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.067679 4797896128 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.068974 4797896128 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.069859 4797896128 tf_logging.py:115] label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.073598 4797896128 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.074545 4797896128 tf_logging.py:115] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] a somewhat typical bit of filmmaking from this era . obviously , it was first conceived into this world for the stage , but nonetheless a very good film from beginning to end . peter o ' tool ##e and susannah york get to do their stage performance act for the silver screen and both do it effectively . there is very little in the way of story and anyone not familiar with this type of off beat character study may be a little put off by it . all in all , though , a good film in which peter o ' tool ##e and susannah york get to over ##act . [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.075512 4797896128 tf_logging.py:115] tokens: [CLS] a somewhat typical bit of filmmaking from this era . obviously , it was first conceived into this world for the stage , but nonetheless a very good film from beginning to end . peter o ' tool ##e and susannah york get to do their stage performance act for the silver screen and both do it effectively . there is very little in the way of story and anyone not familiar with this type of off beat character study may be a little put off by it . all in all , though , a good film in which peter o ' tool ##e and susannah york get to over ##act . [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1037 5399 5171 2978 1997 24466 2013 2023 3690 1012 5525 1010 2009 2001 2034 10141 2046 2023 2088 2005 1996 2754 1010 2021 9690 1037 2200 2204 2143 2013 2927 2000 2203 1012 2848 1051 1005 6994 2063 1998 20471 2259 2131 2000 2079 2037 2754 2836 2552 2005 1996 3165 3898 1998 2119 2079 2009 6464 1012 2045 2003 2200 2210 1999 1996 2126 1997 2466 1998 3087 2025 5220 2007 2023 2828 1997 2125 3786 2839 2817 2089 2022 1037 2210 2404 2125 2011 2009 1012 2035 1999 2035 1010 2295 1010 1037 2204 2143 1999 2029 2848 1051 1005 6994 2063 1998 20471 2259 2131 2000 2058 18908 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.076592 4797896128 tf_logging.py:115] input_ids: 101 1037 5399 5171 2978 1997 24466 2013 2023 3690 1012 5525 1010 2009 2001 2034 10141 2046 2023 2088 2005 1996 2754 1010 2021 9690 1037 2200 2204 2143 2013 2927 2000 2203 1012 2848 1051 1005 6994 2063 1998 20471 2259 2131 2000 2079 2037 2754 2836 2552 2005 1996 3165 3898 1998 2119 2079 2009 6464 1012 2045 2003 2200 2210 1999 1996 2126 1997 2466 1998 3087 2025 5220 2007 2023 2828 1997 2125 3786 2839 2817 2089 2022 1037 2210 2404 2125 2011 2009 1012 2035 1999 2035 1010 2295 1010 1037 2204 2143 1999 2029 2848 1051 1005 6994 2063 1998 20471 2259 2131 2000 2058 18908 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.078057 4797896128 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.078922 4797896128 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:13.080081 4797896128 tf_logging.py:115] label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Writing example 0 of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.262017 4797896128 tf_logging.py:115] Writing example 0 of 5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.266409 4797896128 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.268334 4797896128 tf_logging.py:115] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] there were nice characters in here , played by pleasant - looking actors and actresses , plus it had a famous band and some famous dancers . . . . . yet the film just didn ' t work . by the time this was almost over , i was bored to death . the dial ##og was dumb , the humor ( mainly milton be ##rle ' s ) was down ##right stupid and the music was just not up my alley . < br / > < br / > i ' ve never been a big - band fan , anyway , and if i hear \" in the mood \" one more time i ' ll pu ##ke . < br / [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.269733 4797896128 tf_logging.py:115] tokens: [CLS] there were nice characters in here , played by pleasant - looking actors and actresses , plus it had a famous band and some famous dancers . . . . . yet the film just didn ' t work . by the time this was almost over , i was bored to death . the dial ##og was dumb , the humor ( mainly milton be ##rle ' s ) was down ##right stupid and the music was just not up my alley . < br / > < br / > i ' ve never been a big - band fan , anyway , and if i hear \" in the mood \" one more time i ' ll pu ##ke . < br / [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2045 2020 3835 3494 1999 2182 1010 2209 2011 8242 1011 2559 5889 1998 19910 1010 4606 2009 2018 1037 3297 2316 1998 2070 3297 10487 1012 1012 1012 1012 1012 2664 1996 2143 2074 2134 1005 1056 2147 1012 2011 1996 2051 2023 2001 2471 2058 1010 1045 2001 11471 2000 2331 1012 1996 13764 8649 2001 12873 1010 1996 8562 1006 3701 9660 2022 20927 1005 1055 1007 2001 2091 15950 5236 1998 1996 2189 2001 2074 2025 2039 2026 8975 1012 1026 7987 1013 1028 1026 7987 1013 1028 1045 1005 2310 2196 2042 1037 2502 1011 2316 5470 1010 4312 1010 1998 2065 1045 2963 1000 1999 1996 6888 1000 2028 2062 2051 1045 1005 2222 16405 3489 1012 1026 7987 1013 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.270946 4797896128 tf_logging.py:115] input_ids: 101 2045 2020 3835 3494 1999 2182 1010 2209 2011 8242 1011 2559 5889 1998 19910 1010 4606 2009 2018 1037 3297 2316 1998 2070 3297 10487 1012 1012 1012 1012 1012 2664 1996 2143 2074 2134 1005 1056 2147 1012 2011 1996 2051 2023 2001 2471 2058 1010 1045 2001 11471 2000 2331 1012 1996 13764 8649 2001 12873 1010 1996 8562 1006 3701 9660 2022 20927 1005 1055 1007 2001 2091 15950 5236 1998 1996 2189 2001 2074 2025 2039 2026 8975 1012 1026 7987 1013 1028 1026 7987 1013 1028 1045 1005 2310 2196 2042 1037 2502 1011 2316 5470 1010 4312 1010 1998 2065 1045 2963 1000 1999 1996 6888 1000 2028 2062 2051 1045 1005 2222 16405 3489 1012 1026 7987 1013 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.272295 4797896128 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.273684 4797896128 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.274955 4797896128 tf_logging.py:115] label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.280347 4797896128 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.281509 4797896128 tf_logging.py:115] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] for a long time , i , a fan of \" the monk ##ees \" tv series , refused to watch \" head \" because it was not about the tv show characters , who were warm and wonderful . \" head \" , instead , was said to be a cynical , dark movie . finally , curiosity caused me to cave in . i didn ' t , of course , find a new episode to the tv show , but a fascinating movie that appeals to my dark side . < br / > < br / > i have always been fascinated by dreams , and \" head \" was very much like watching someone else ' s dream , with incomplete [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.283039 4797896128 tf_logging.py:115] tokens: [CLS] for a long time , i , a fan of \" the monk ##ees \" tv series , refused to watch \" head \" because it was not about the tv show characters , who were warm and wonderful . \" head \" , instead , was said to be a cynical , dark movie . finally , curiosity caused me to cave in . i didn ' t , of course , find a new episode to the tv show , but a fascinating movie that appeals to my dark side . < br / > < br / > i have always been fascinated by dreams , and \" head \" was very much like watching someone else ' s dream , with incomplete [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2005 1037 2146 2051 1010 1045 1010 1037 5470 1997 1000 1996 8284 10285 1000 2694 2186 1010 4188 2000 3422 1000 2132 1000 2138 2009 2001 2025 2055 1996 2694 2265 3494 1010 2040 2020 4010 1998 6919 1012 1000 2132 1000 1010 2612 1010 2001 2056 2000 2022 1037 26881 1010 2601 3185 1012 2633 1010 10628 3303 2033 2000 5430 1999 1012 1045 2134 1005 1056 1010 1997 2607 1010 2424 1037 2047 2792 2000 1996 2694 2265 1010 2021 1037 17160 3185 2008 9023 2000 2026 2601 2217 1012 1026 7987 1013 1028 1026 7987 1013 1028 1045 2031 2467 2042 15677 2011 5544 1010 1998 1000 2132 1000 2001 2200 2172 2066 3666 2619 2842 1005 1055 3959 1010 2007 12958 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.284322 4797896128 tf_logging.py:115] input_ids: 101 2005 1037 2146 2051 1010 1045 1010 1037 5470 1997 1000 1996 8284 10285 1000 2694 2186 1010 4188 2000 3422 1000 2132 1000 2138 2009 2001 2025 2055 1996 2694 2265 3494 1010 2040 2020 4010 1998 6919 1012 1000 2132 1000 1010 2612 1010 2001 2056 2000 2022 1037 26881 1010 2601 3185 1012 2633 1010 10628 3303 2033 2000 5430 1999 1012 1045 2134 1005 1056 1010 1997 2607 1010 2424 1037 2047 2792 2000 1996 2694 2265 1010 2021 1037 17160 3185 2008 9023 2000 2026 2601 2217 1012 1026 7987 1013 1028 1026 7987 1013 1028 1045 2031 2467 2042 15677 2011 5544 1010 1998 1000 2132 1000 2001 2200 2172 2066 3666 2619 2842 1005 1055 3959 1010 2007 12958 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.285623 4797896128 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.287049 4797896128 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.288587 4797896128 tf_logging.py:115] label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.292611 4797896128 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.294513 4797896128 tf_logging.py:115] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] c . h . o . m . p . s . is very much like any number of che ##es ##y late 70s disney family comedy ##s - the cat from outer space or unidentified flying odd ##ball , for instance . utterly devoid of anything creative , beating the same cl ##iche ##s to death , yet vaguely entertaining in a mind ##less sort of way . the actors won ' t win any awards , nor will the director , writer , or fx crew , but in its in ##off ##ens ##ive ness and bland pre ##dic ##ata ##bility there is some vague entertainment to be had . the idea of the robot dog as security system is so full of holes [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.295943 4797896128 tf_logging.py:115] tokens: [CLS] c . h . o . m . p . s . is very much like any number of che ##es ##y late 70s disney family comedy ##s - the cat from outer space or unidentified flying odd ##ball , for instance . utterly devoid of anything creative , beating the same cl ##iche ##s to death , yet vaguely entertaining in a mind ##less sort of way . the actors won ' t win any awards , nor will the director , writer , or fx crew , but in its in ##off ##ens ##ive ness and bland pre ##dic ##ata ##bility there is some vague entertainment to be had . the idea of the robot dog as security system is so full of holes [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 1039 1012 1044 1012 1051 1012 1049 1012 1052 1012 1055 1012 2003 2200 2172 2066 2151 2193 1997 18178 2229 2100 2397 17549 6373 2155 4038 2015 1011 1996 4937 2013 6058 2686 2030 20293 3909 5976 7384 1010 2005 6013 1012 12580 22808 1997 2505 5541 1010 6012 1996 2168 18856 17322 2015 2000 2331 1010 2664 15221 14036 1999 1037 2568 3238 4066 1997 2126 1012 1996 5889 2180 1005 1056 2663 2151 2982 1010 4496 2097 1996 2472 1010 3213 1010 2030 23292 3626 1010 2021 1999 2049 1999 7245 6132 3512 23384 1998 20857 3653 14808 6790 8553 2045 2003 2070 13727 4024 2000 2022 2018 1012 1996 2801 1997 1996 8957 3899 2004 3036 2291 2003 2061 2440 1997 8198 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.297461 4797896128 tf_logging.py:115] input_ids: 101 1039 1012 1044 1012 1051 1012 1049 1012 1052 1012 1055 1012 2003 2200 2172 2066 2151 2193 1997 18178 2229 2100 2397 17549 6373 2155 4038 2015 1011 1996 4937 2013 6058 2686 2030 20293 3909 5976 7384 1010 2005 6013 1012 12580 22808 1997 2505 5541 1010 6012 1996 2168 18856 17322 2015 2000 2331 1010 2664 15221 14036 1999 1037 2568 3238 4066 1997 2126 1012 1996 5889 2180 1005 1056 2663 2151 2982 1010 4496 2097 1996 2472 1010 3213 1010 2030 23292 3626 1010 2021 1999 2049 1999 7245 6132 3512 23384 1998 20857 3653 14808 6790 8553 2045 2003 2070 13727 4024 2000 2022 2018 1012 1996 2801 1997 1996 8957 3899 2004 3036 2291 2003 2061 2440 1997 8198 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.300220 4797896128 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.302360 4797896128 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.303723 4797896128 tf_logging.py:115] label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.307429 4797896128 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.308779 4797896128 tf_logging.py:115] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] nothing short of magnificent photography / cinematography in this film . the fact that you keep seeking and hoping for more flying sequences , tells you that they have just enough . the acting is fantastic , the stories are seam ##lessly woven together , and the dogs are splendid . . . . . . . . . . . . . < br / > < br / > a must rent , view , or see . < br / > < br / > don ' t be afraid of sub ##titles . . . . . . . . < br / > < br / > its worth a little ave ##rs ##ion therapy < br / > < br [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.309818 4797896128 tf_logging.py:115] tokens: [CLS] nothing short of magnificent photography / cinematography in this film . the fact that you keep seeking and hoping for more flying sequences , tells you that they have just enough . the acting is fantastic , the stories are seam ##lessly woven together , and the dogs are splendid . . . . . . . . . . . . . < br / > < br / > a must rent , view , or see . < br / > < br / > don ' t be afraid of sub ##titles . . . . . . . . < br / > < br / > its worth a little ave ##rs ##ion therapy < br / > < br [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2498 2460 1997 12047 5855 1013 16434 1999 2023 2143 1012 1996 2755 2008 2017 2562 6224 1998 5327 2005 2062 3909 10071 1010 4136 2017 2008 2027 2031 2074 2438 1012 1996 3772 2003 10392 1010 1996 3441 2024 25180 10895 17374 2362 1010 1998 1996 6077 2024 21459 1012 1012 1012 1012 1012 1012 1012 1012 1012 1012 1012 1012 1012 1026 7987 1013 1028 1026 7987 1013 1028 1037 2442 9278 1010 3193 1010 2030 2156 1012 1026 7987 1013 1028 1026 7987 1013 1028 2123 1005 1056 2022 4452 1997 4942 27430 1012 1012 1012 1012 1012 1012 1012 1012 1026 7987 1013 1028 1026 7987 1013 1028 2049 4276 1037 2210 13642 2869 3258 7242 1026 7987 1013 1028 1026 7987 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.311062 4797896128 tf_logging.py:115] input_ids: 101 2498 2460 1997 12047 5855 1013 16434 1999 2023 2143 1012 1996 2755 2008 2017 2562 6224 1998 5327 2005 2062 3909 10071 1010 4136 2017 2008 2027 2031 2074 2438 1012 1996 3772 2003 10392 1010 1996 3441 2024 25180 10895 17374 2362 1010 1998 1996 6077 2024 21459 1012 1012 1012 1012 1012 1012 1012 1012 1012 1012 1012 1012 1012 1026 7987 1013 1028 1026 7987 1013 1028 1037 2442 9278 1010 3193 1010 2030 2156 1012 1026 7987 1013 1028 1026 7987 1013 1028 2123 1005 1056 2022 4452 1997 4942 27430 1012 1012 1012 1012 1012 1012 1012 1012 1026 7987 1013 1028 1026 7987 1013 1028 2049 4276 1037 2210 13642 2869 3258 7242 1026 7987 1013 1028 1026 7987 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.312147 4797896128 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.313669 4797896128 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.314795 4797896128 tf_logging.py:115] label: 1 (id = 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Example ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.319355 4797896128 tf_logging.py:115] *** Example ***\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:guid: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.320523 4797896128 tf_logging.py:115] guid: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens: [CLS] only watched this to see joe morton in an early role and honestly wished i hadn ' t bothered , he can and has since , done much better than this crap . cannot understand why anyone finds this kind of stupidity funny but each to his own ; it is an absolute mess and not funny in the least . no wait , one line only was funny , where mr kent ( joe ) and his family are having dinner with this nut job as he ' s been invited for dinner ( lord alone knows why ) . pest to mr kent : you know what it ' s like dog , you ' ve been there mrs kent : not lately , [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.321554 4797896128 tf_logging.py:115] tokens: [CLS] only watched this to see joe morton in an early role and honestly wished i hadn ' t bothered , he can and has since , done much better than this crap . cannot understand why anyone finds this kind of stupidity funny but each to his own ; it is an absolute mess and not funny in the least . no wait , one line only was funny , where mr kent ( joe ) and his family are having dinner with this nut job as he ' s been invited for dinner ( lord alone knows why ) . pest to mr kent : you know what it ' s like dog , you ' ve been there mrs kent : not lately , [SEP]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_ids: 101 2069 3427 2023 2000 2156 3533 11164 1999 2019 2220 2535 1998 9826 6257 1045 2910 1005 1056 11250 1010 2002 2064 1998 2038 2144 1010 2589 2172 2488 2084 2023 10231 1012 3685 3305 2339 3087 4858 2023 2785 1997 28072 6057 2021 2169 2000 2010 2219 1025 2009 2003 2019 7619 6752 1998 2025 6057 1999 1996 2560 1012 2053 3524 1010 2028 2240 2069 2001 6057 1010 2073 2720 5982 1006 3533 1007 1998 2010 2155 2024 2383 4596 2007 2023 17490 3105 2004 2002 1005 1055 2042 4778 2005 4596 1006 2935 2894 4282 2339 1007 1012 20739 2000 2720 5982 1024 2017 2113 2054 2009 1005 1055 2066 3899 1010 2017 1005 2310 2042 2045 3680 5982 1024 2025 9906 1010 102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.322627 4797896128 tf_logging.py:115] input_ids: 101 2069 3427 2023 2000 2156 3533 11164 1999 2019 2220 2535 1998 9826 6257 1045 2910 1005 1056 11250 1010 2002 2064 1998 2038 2144 1010 2589 2172 2488 2084 2023 10231 1012 3685 3305 2339 3087 4858 2023 2785 1997 28072 6057 2021 2169 2000 2010 2219 1025 2009 2003 2019 7619 6752 1998 2025 6057 1999 1996 2560 1012 2053 3524 1010 2028 2240 2069 2001 6057 1010 2073 2720 5982 1006 3533 1007 1998 2010 2155 2024 2383 4596 2007 2023 17490 3105 2004 2002 1005 1055 2042 4778 2005 4596 1006 2935 2894 4282 2339 1007 1012 20739 2000 2720 5982 1024 2017 2113 2054 2009 1005 1055 2066 3899 1010 2017 1005 2310 2042 2045 3680 5982 1024 2025 9906 1010 102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.324302 4797896128 tf_logging.py:115] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.326015 4797896128 tf_logging.py:115] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:label: 0 (id = 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0108 21:33:32.327230 4797896128 tf_logging.py:115] label: 0 (id = 0)\n"
     ]
    }
   ],
   "source": [
    "# We'll set sequences to be at most 128 tokens long.\n",
    "MAX_SEQ_LENGTH = 128\n",
    "# Convert our train and test features to InputFeatures that BERT understands.\n",
    "train_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n",
    "test_features = bert.run_classifier.convert_examples_to_features(test_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n",
    "                 num_labels):\n",
    "  \"\"\"Creates a classification model.\"\"\"\n",
    "\n",
    "  bert_module = hub.Module(\n",
    "      BERT_MODEL_HUB,\n",
    "      trainable=True)\n",
    "  bert_inputs = dict(\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      segment_ids=segment_ids)\n",
    "  bert_outputs = bert_module(\n",
    "      inputs=bert_inputs,\n",
    "      signature=\"tokens\",\n",
    "      as_dict=True)\n",
    "\n",
    "  # Use \"pooled_output\" for classification tasks on an entire sentence.\n",
    "  # Use \"sequence_outputs\" for token-level output.\n",
    "  output_layer = bert_outputs[\"pooled_output\"]\n",
    "\n",
    "  hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "  # Create our own layer to tune for politeness data.\n",
    "  output_weights = tf.get_variable(\n",
    "      \"output_weights\", [num_labels, hidden_size],\n",
    "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "  output_bias = tf.get_variable(\n",
    "      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "  with tf.variable_scope(\"loss\"):\n",
    "\n",
    "    # Dropout helps prevent overfitting\n",
    "    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "\n",
    "    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "    logits = tf.nn.bias_add(logits, output_bias)\n",
    "    log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "\n",
    "    # Convert labels into one-hot encoding\n",
    "    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "\n",
    "    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n",
    "    # If we're predicting, we want predicted labels and the probabiltiies.\n",
    "    if is_predicting:\n",
    "      return (predicted_labels, log_probs)\n",
    "\n",
    "    # If we're train/eval, compute loss between predicted and actual label\n",
    "    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "    loss = tf.reduce_mean(per_example_loss)\n",
    "    return (loss, predicted_labels, log_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_fn_builder actually creates our model function\n",
    "# using the passed parameters for num_labels, learning_rate, etc.\n",
    "def model_fn_builder(num_labels, learning_rate, num_train_steps,\n",
    "                     num_warmup_steps):\n",
    "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "    label_ids = features[\"label_ids\"]\n",
    "\n",
    "    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n",
    "    \n",
    "    # TRAIN and EVAL\n",
    "    if not is_predicting:\n",
    "\n",
    "      (loss, predicted_labels, log_probs) = create_model(\n",
    "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "      train_op = bert.optimization.create_optimizer(\n",
    "          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
    "\n",
    "      # Calculate evaluation metrics. \n",
    "      def metric_fn(label_ids, predicted_labels):\n",
    "        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "        f1_score = tf.contrib.metrics.f1_score(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        auc = tf.metrics.auc(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        recall = tf.metrics.recall(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        precision = tf.metrics.precision(\n",
    "            label_ids,\n",
    "            predicted_labels) \n",
    "        true_pos = tf.metrics.true_positives(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        true_neg = tf.metrics.true_negatives(\n",
    "            label_ids,\n",
    "            predicted_labels)   \n",
    "        false_pos = tf.metrics.false_positives(\n",
    "            label_ids,\n",
    "            predicted_labels)  \n",
    "        false_neg = tf.metrics.false_negatives(\n",
    "            label_ids,\n",
    "            predicted_labels)\n",
    "        return {\n",
    "            \"eval_accuracy\": accuracy,\n",
    "            \"f1_score\": f1_score,\n",
    "            \"auc\": auc,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"true_positives\": true_pos,\n",
    "            \"true_negatives\": true_neg,\n",
    "            \"false_positives\": false_pos,\n",
    "            \"false_negatives\": false_neg\n",
    "        }\n",
    "\n",
    "      eval_metrics = metric_fn(label_ids, predicted_labels)\n",
    "\n",
    "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "          loss=loss,\n",
    "          train_op=train_op)\n",
    "      else:\n",
    "          return tf.estimator.EstimatorSpec(mode=mode,\n",
    "            loss=loss,\n",
    "            eval_metric_ops=eval_metrics)\n",
    "    else:\n",
    "      (predicted_labels, log_probs) = create_model(\n",
    "        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n",
    "\n",
    "      predictions = {\n",
    "          'probabilities': log_probs,\n",
    "          'labels': predicted_labels\n",
    "      }\n",
    "      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "  # Return the actual model function in the closure\n",
    "  return model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
